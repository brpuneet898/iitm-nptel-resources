{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "Policy Iteration is a DP Algorithm to find the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 4, 4\n",
    "r, c = 0, 0\n",
    "S = [(r, c) for i in range(m) for c in range(n)]\n",
    "A = ['north', 'south', 'east', 'west']\n",
    "acount = len(A)\n",
    "terminal = [(0,0),(3,3)]\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(s, a, s_next):\n",
    "    r, c = s\n",
    "    if a == 'north':\n",
    "        r = r if r == 0 else r - 1\n",
    "    if a == 'south':\n",
    "        r = r if r == m - 1 else r + 1\n",
    "    if a == 'west':\n",
    "        c = c if c == 0 else c - 1\n",
    "    if a == 'east':\n",
    "        c = c if c == n - 1 else c + 1\n",
    "    prob = 1 if (r, c) == s_next else 0\n",
    "    reward = -1\n",
    "    return prob, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using equiprobable policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = {s: {a: 0.25 for a in A} for s in S}\n",
    "pi[(0, 2)]['east'] ## Example to show the equiprobable policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at two kinds of updates - synchronous and in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations for Convergence: 295\n",
      "Value Function Rounded Off to Nearest Integer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0., nan, nan, nan],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_numpy(v, m, n):\n",
    "    vnum = np.zeros((m, n))\n",
    "    for s in v:\n",
    "        vnum[s] = v[s]\n",
    "    return vnum\n",
    "\n",
    "theta = 1e-3\n",
    "delta = theta + 1\n",
    "v = {s: 0 for s in S}\n",
    "iter = 0\n",
    "\n",
    "while delta >= theta:\n",
    "    iter += 1\n",
    "    v_next = {s: 0 for s in S}\n",
    "    delta = 0\n",
    "    for s in S:\n",
    "        if s in terminal:\n",
    "            continue\n",
    "        for a in A:\n",
    "            for s_next in S:\n",
    "                prob, reward = transition(s, a, s_next)\n",
    "                v_next[s] += pi[s][a] * prob * (reward + gamma * v[s_next])\n",
    "        delta = max(delta, abs(v[s] - v_next[s]))\n",
    "    v = v_next.copy()\n",
    "v = {s: np.round(v[s]) for s in S}\n",
    "\n",
    "print(f'Number of Iterations for Convergence: {iter}')\n",
    "print(f'Value Function Rounded Off to Nearest Integer')\n",
    "to_numpy(v, m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Place Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations for Convergence: 1024\n",
      "Value Funciton Rounded Off to Nearest Integer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., nan],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(pi):\n",
    "    theta = 1e-3\n",
    "    delta = theta + 1\n",
    "    v = {s: 0 for s in S}\n",
    "    iter = 0\n",
    "    while delta >= theta:\n",
    "        iter += 1\n",
    "        delta = 0\n",
    "        for s in S:\n",
    "            vs = 0\n",
    "        if s in terminal:\n",
    "            continue\n",
    "        for a in A:\n",
    "            for s_next in S:\n",
    "                prob, reward = transition(s, a, s_next)\n",
    "                vs += pi[s][a] * prob * (reward + gamma * v[s_next])\n",
    "        delta = max(delta, abs(v[s] - vs))\n",
    "        v[s] = vs\n",
    "    return v, iter\n",
    "v, iter = evaluate(pi)\n",
    "v = {s: np.round(v[s]) for s in S}\n",
    "print(f'Number of Iterations for Convergence: {iter}')\n",
    "print(\"Value Funciton Rounded Off to Nearest Integer\")\n",
    "to_numpy(v, m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve(v):\n",
    "    pi_next = {s: {a: 0 for a in A} for s in S}\n",
    "    q = {s: {a: 0 for a in A} for s in S}\n",
    "    for s in S:\n",
    "        for a in A:\n",
    "            for s_next in S:\n",
    "                prob, reward = transition(s, a, s_next)\n",
    "                q[s][a] += prob * (reward + gamma * v[s_next])\n",
    "        amax = max([(a, q[s][a]) for a in A], key = lambda x: x[1])[0]\n",
    "        pi_next[s][amax] = 1\n",
    "    return pi_next\n",
    "pi_next = improve(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_next = {s: {a: 1/acount for a in A} for s in S}\n",
    "pi = {s: {a: 1 if a == 'north' else 0 for a in A} for s in S}\n",
    "\n",
    "iter = 0\n",
    "while not np.array_equal(pi, pi_next):\n",
    "    pi = pi_next.copy()\n",
    "    v, _ = evaluate(pi)\n",
    "    pi_next = improve(v)\n",
    "    iter += 1\n",
    "\n",
    "print(f'Number of Iterations for Convergence: {iter}')\n",
    "to_numpy(v, m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [6, 6]\n",
    "ext = 0.1\n",
    "arrow_map = {'north': (0, ext), 'south': (0, -ext), 'east': (ext, 0), 'west': (-ext, 0)}\n",
    "q = {s: {a: 0 for a in A} for s in S}\n",
    "for s in S:\n",
    "    if s in terminal:\n",
    "        continue\n",
    "    for a in A:\n",
    "        for s_next in S:\n",
    "            prob, reward = transition(s, a, s_next)\n",
    "            q[s][a] += prob * (reward + gamma * v[s_next])\n",
    "    maxq = max([q[s][a] for a in A])\n",
    "    opt_acts = [a for a in A if q[s][a] == maxq]\n",
    "    for a in opt_acts:\n",
    "        r, c = s\n",
    "        x, y = c + 0.5, m - 1 - r + 0.5,\n",
    "        plt.arrow(x, y, arrow_map[a][0], arrow_map[a][1], head_width = 0.1, head_length = 0.1)\n",
    "        plt.text(x - 0.4, y - 0.4, f'${v[s]}$')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlim([0, n])\n",
    "plt.ylim([0, m])\n",
    "plt.xticks(np.arrange(0, n, 1), np.arrange(1, n+1))\n",
    "plt.yticks(np.arrange(0, m, 1), np.arrange(1, m+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
