{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:06:14.252592Z","iopub.execute_input":"2025-02-15T17:06:14.252875Z","iopub.status.idle":"2025-02-15T17:06:14.707443Z","shell.execute_reply.started":"2025-02-15T17:06:14.252846Z","shell.execute_reply":"2025-02-15T17:06:14.706576Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\n/kaggle/input/multi-lingual-sentiment-analysis/train.csv\n/kaggle/input/multi-lingual-sentiment-analysis/test.csv\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\n# Load the train and test data\ntrain_data = pd.read_csv('/kaggle/input/multi-lingual-sentiment-analysis/train.csv')\ntest_data = pd.read_csv('/kaggle/input/multi-lingual-sentiment-analysis/test.csv')\n\n# Display first few rows of the train and test data\nprint(\"Train Data:\")\nprint(train_data.head())\n\nprint(\"\\nTest Data:\")\nprint(test_data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:06:14.708179Z","iopub.execute_input":"2025-02-15T17:06:14.708556Z","iopub.status.idle":"2025-02-15T17:06:14.750451Z","shell.execute_reply.started":"2025-02-15T17:06:14.708529Z","shell.execute_reply":"2025-02-15T17:06:14.749594Z"}},"outputs":[{"name":"stdout","text":"Train Data:\n   ID                                           sentence     label language\n0   1  কর্মীদের ভাল আচরণ এবং খাবারের পাশাপাশি পানীয় ...  Positive       bn\n1   2  ગોદરેજ સેન્ટ્રલ એસીમાં તેના કન્ડેન્સર પર 2 વર્...  Positive       gu\n2   3  கதைக்களம் பிடித்திருந்தது, அனைத்து நடிகர்களும்...  Positive       ta\n3   4  ਵੌਇਸ-ਓਵਰ ਬਹੁਤ ਵਧੀਆ ਸੀ ਅਤੇ ਕਹਾਣੀ ਦੀ ਸੀਮਾ ਵਿੱਚ ਇ...  Positive       pa\n4   5  जुथानि थाखाय जायगा गैया। गुबुन मुवा सोग्रा जाय...  Negative       bd\n\nTest Data:\n   ID                                           sentence language\n0   1                    1120 mAh, ਓਵਰਚਾਰਜਿੰਗ ਦੀ ਸੁਰੱਖਿਆ       pa\n1   2  તે સઘન મોઇશ્ચરાઇઝિંગ પ્રદાન કરે છે અને સરસ સ્વ...       gu\n2   3                      1120 ಎಂಎಎಚ್, ಮಿತಿಮೀರಿದ ರಕ್ಷಣೆ       kn\n3   4  ভাৰতত নিৰ্মিত সৰ্বশ্ৰেষ্ঠ পাৰফিউম ব্ৰেণ্ডবোৰৰ ...       as\n4   5  میں نے حال ہی میں \"انفولڈ\" سے ایک ٹیمپلیٹ خرید...       ur\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Define x_train, y_train, x_test\nx_train = train_data['sentence']\ny_train = train_data['label']\nx_test = test_data['sentence']\n\n# Check the shapes to verify\nprint(f\"x_train shape: {x_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"x_test shape: {x_test.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:06:14.751305Z","iopub.execute_input":"2025-02-15T17:06:14.751593Z","iopub.status.idle":"2025-02-15T17:06:14.757004Z","shell.execute_reply.started":"2025-02-15T17:06:14.751566Z","shell.execute_reply":"2025-02-15T17:06:14.756050Z"}},"outputs":[{"name":"stdout","text":"x_train shape: (1000,)\ny_train shape: (1000,)\nx_test shape: (100,)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\n\n# Check if CUDA (GPU) is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:06:14.757786Z","iopub.execute_input":"2025-02-15T17:06:14.758065Z","iopub.status.idle":"2025-02-15T17:06:18.201768Z","shell.execute_reply.started":"2025-02-15T17:06:14.758038Z","shell.execute_reply":"2025-02-15T17:06:18.200640Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\n\n# Path to the model directory\nmodel_id = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\n# Create a text-generation pipeline with optimized memory usage\npipeline_generator = pipeline(\n    \"text-generation\", \n    model=model_id, \n    model_kwargs={\"torch_dtype\": torch.bfloat16},  # Use bfloat16 for reduced memory usage\n    device_map=\"balanced\",  # Try balancing between CPU and GPU\n    truncation=True,  # Explicitly activate truncation\n    pad_token_id=128001  # Ensure padding uses the EOS token (128001)\n)\n\n\n# Test the pipeline with an example input\noutput = pipeline_generator(\"Once upon a time,\")\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:06:18.203117Z","iopub.execute_input":"2025-02-15T17:06:18.203683Z","iopub.status.idle":"2025-02-15T17:08:39.677664Z","shell.execute_reply.started":"2025-02-15T17:06:18.203643Z","shell.execute_reply":"2025-02-15T17:08:39.676784Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eefec39efe1a4e1890b73be87478e8f0"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"[{'generated_text': 'Once upon a time, in a world not so different from our own, there lived a young woman named Sophia. Sophia was'}]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:08:39.679652Z","iopub.execute_input":"2025-02-15T17:08:39.680138Z","iopub.status.idle":"2025-02-15T17:08:39.684987Z","shell.execute_reply.started":"2025-02-15T17:08:39.680112Z","shell.execute_reply":"2025-02-15T17:08:39.684041Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def get_sentiment(sentence):\n    prompt = f\"Please, for god's sake classify the sentiment of the following sentence as Positive or Negative: {sentence}\"\n    output = pipeline_generator(prompt, max_new_tokens=10, num_return_sequences=1)\n    response = output[0]['generated_text'].strip().lower()\n    # Parse the output to return \"Positive\" or \"Negative\"\n    if \"positive\" in response:\n        return \"Positive\"\n    else:\n        return \"Negative\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:08:39.686322Z","iopub.execute_input":"2025-02-15T17:08:39.686677Z","iopub.status.idle":"2025-02-15T17:08:39.699197Z","shell.execute_reply.started":"2025-02-15T17:08:39.686643Z","shell.execute_reply":"2025-02-15T17:08:39.698383Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:08:39.700258Z","iopub.execute_input":"2025-02-15T17:08:39.700588Z","iopub.status.idle":"2025-02-15T17:08:39.712704Z","shell.execute_reply.started":"2025-02-15T17:08:39.700556Z","shell.execute_reply":"2025-02-15T17:08:39.712036Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"    # predictions = []\n    \n    # # Loop through each row in the test_data and get the sentiment of the sentence\n    # for _, row in test_data.iterrows():\n    #     sentence = row['sentence']  # Assuming the column name containing sentences is 'sentence'\n    #     sentiment = get_sentiment(sentence)\n    #     predictions.append(sentiment)\n    \n    # # Create a new DataFrame for the submission\n    # submission_df = pd.DataFrame({\n    #     'ID': test_data['ID'],  # Assuming 'ID' column exists in the test data\n    #     'label': predictions\n    # })\n    \n    # # Save the submission to a CSV file\n    # submission_df.to_csv('submission.csv', index=False)\n    \n    # print(\"Submission file created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:08:39.713389Z","iopub.execute_input":"2025-02-15T17:08:39.713572Z","iopub.status.idle":"2025-02-15T17:08:39.724599Z","shell.execute_reply.started":"2025-02-15T17:08:39.713556Z","shell.execute_reply":"2025-02-15T17:08:39.723873Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"sentences = test_data['sentence'].tolist()\n\n# Process the sentences in batches while using the custom prompt\ndef batch_get_sentiment_with_prompt(sentences, batch_size=32):\n    results = []\n    for i in range(0, len(sentences), batch_size):\n        batch = sentences[i:i + batch_size]\n        \n        # Prepare the prompts for each sentence\n        prompts = [f\"Please, for god's sake classify the sentiment of the following sentence as Positive or Negative: {sentence}\" for sentence in batch]\n        \n        # Get the model's output for all the prompts in one batch\n        batch_output = pipeline_generator(prompts, max_new_tokens=10, num_return_sequences=1)\n\n        # Process the output and classify sentiment\n        for output in batch_output:\n            sentiment = output[0]['generated_text'].strip().lower()\n            if \"positive\" in sentiment:\n                results.append(\"Positive\")\n            else:\n                results.append(\"Negative\")\n    \n    return results\n\n# Get the sentiments in batches using the custom prompt engineering\nsentiments = batch_get_sentiment_with_prompt(sentences)\n\n# Create the submission DataFrame\nsubmission_df = pd.DataFrame({\n    'ID': test_data['ID'],\n    'label': sentiments\n})\n\n# Save the results to a CSV file\nsubmission_df.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"Submission file created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:08:39.725317Z","iopub.execute_input":"2025-02-15T17:08:39.725572Z","iopub.status.idle":"2025-02-15T17:12:05.351167Z","shell.execute_reply.started":"2025-02-15T17:08:39.725542Z","shell.execute_reply":"2025-02-15T17:12:05.350297Z"}},"outputs":[{"name":"stdout","text":"Submission file created successfully!\n","output_type":"stream"}],"execution_count":10}]}