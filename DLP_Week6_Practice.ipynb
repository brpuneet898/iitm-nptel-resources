{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install speechbrain==0.5.16\n",
    "!pip install faster-whisper\n",
    "!pip install pyannote.studio\n",
    "!pip install whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import traceback\n",
    "from faster_whisper import WhisperModel\n",
    "import torch\n",
    "import whisper\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment\n",
    "import speechbrain\n",
    "from scipy.spatial.distance import cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the audio file in your colab notebook\n",
    "audio_file_path = '/content/TEST-1.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: convert mp3 to wav format\n",
    "\n",
    "# !ffmpeg -i \"{audio_file_path}\" \"{audio_file_path[:-4]}.wav\"\n",
    "!ffmpeg -i \"{audio_file_path}\" -ar 16000 -ac 1 -c:a pcm_s16le \"{audio_file_path[:-4]}.wav\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "whisper_models = [\"tiny\", \"base\", \"small\", \"medium\", \"large-v1\", \"large-v2\"]\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = PretrainedSpeakerEmbedding(\n",
    "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "\n",
    "def convert_time(secs):\n",
    "    return datetime.timedelta(seconds=round(secs))\n",
    "\n",
    "def speech_to_text(audio_file, whisper_model):\n",
    "    model = WhisperModel(whisper_model, compute_type=\"int8\")\n",
    "    time_start = time.time()\n",
    "\n",
    "    try:\n",
    "        # Get duration\n",
    "        audio_data, sample_rate = librosa.load(audio_file, mono= True, sr=16000)\n",
    "        duration = len(audio_data) / sample_rate  # Calculate duration\n",
    "\n",
    "        # Transcribe audio\n",
    "        options = dict(language='en', beam_size=5, best_of=5)\n",
    "        transcribe_options = dict(task=\"transcribe\", **options)\n",
    "        segments_raw, info = model.transcribe(audio_file, **transcribe_options)\n",
    "\n",
    "        # Convert back to original openai format\n",
    "        segments = []\n",
    "        for segment_chunk in segments_raw:\n",
    "            chunk = {}\n",
    "            chunk[\"start\"] = segment_chunk.start\n",
    "            chunk[\"end\"] = segment_chunk.end\n",
    "            segments.append(chunk)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Error converting video to audio\")\n",
    "\n",
    "    try:\n",
    "        # Create embedding\n",
    "        def segment_embedding(segment):\n",
    "          try:\n",
    "              audio = Audio()\n",
    "              start = segment[\"start\"]\n",
    "              end = min(duration, segment[\"end\"])\n",
    "\n",
    "              clip = Segment(start, end)\n",
    "              waveform, sample_rate = audio.crop(audio_file, clip)\n",
    "\n",
    "              embeddings = embedding_model(waveform[None])\n",
    "\n",
    "              return embeddings\n",
    "          except Exception as e:\n",
    "              traceback.print_exc()\n",
    "              raise RuntimeError(\"Error during segment embedding\", e)\n",
    "\n",
    "\n",
    "        # Create embedding\n",
    "        embeddings = np.zeros(shape=(len(segments), 192))\n",
    "        for i, segment in enumerate(segments):\n",
    "            embeddings[i] = segment_embedding(segment)\n",
    "        embeddings = np.nan_to_num(embeddings)\n",
    "\n",
    "\n",
    "        # Assign speaker label\n",
    "        best_num_speaker = 2\n",
    "        clustering = AgglomerativeClustering(best_num_speaker).fit(embeddings)\n",
    "        labels = clustering.labels_\n",
    "        for i in range(len(segments)):\n",
    "            segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
    "\n",
    "        # Make output\n",
    "        objects = {\n",
    "            'Start' : [],\n",
    "            'End': [],\n",
    "            'Speaker': [],\n",
    "        }\n",
    "        for (i, segment) in enumerate(segments):\n",
    "            if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
    "                objects['Start'].append(str(convert_time(segment[\"start\"])))\n",
    "                objects['Speaker'].append(segment[\"speaker\"])\n",
    "                if i != 0:\n",
    "                    objects['End'].append(str(convert_time(segments[i - 1][\"end\"])))\n",
    "        objects['End'].append(str(convert_time(segments[i - 1][\"end\"])))\n",
    "\n",
    "        save_path = \"/content/TEST-1.csv\"\n",
    "        df_results = pd.DataFrame(objects)\n",
    "        df_results.to_csv(save_path)\n",
    "        return df_results, save_path\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print exception for debugging\n",
    "        print(\"Exception occurred:\", e)\n",
    "        raise RuntimeError(\"Error Running inference with local model\", e)\n",
    "\n",
    "\n",
    "# Provide the path to your audio file\n",
    "audio_file = \"/content/TEST-1.wav\"\n",
    "\n",
    "# Set the Whisper model and number of speakers\n",
    "selected_whisper_model = \"base\"\n",
    "\n",
    "# Run the transcription\n",
    "transcription_results, save_path = speech_to_text(audio_file, selected_whisper_model)\n",
    "\n",
    "# Print the transcription results\n",
    "print(transcription_results)\n",
    "print(f\"Transcription results saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install moviepy pandas pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "\n",
    "df_results = transcription_results\n",
    "\n",
    "# Step 4: Load the Video\n",
    "video_path = '/content/videoplayback_test1.mp4'  # Update with your video path\n",
    "video = VideoFileClip(video_path)\n",
    "\n",
    "# Function to create an image with text\n",
    "def create_text_image(text, font_size=70, img_size=(640, 80), bg_color=(0, 0, 0), text_color=(255, 255, 255)):\n",
    "    img = Image.new('RGB', img_size, color=bg_color)\n",
    "    d = ImageDraw.Draw(img)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    text_width, text_height = d.textsize(text, font=font)\n",
    "    position = ((img_size[0]-text_width)/2, (img_size[1]-text_height)/2)\n",
    "    d.text(position, text, fill=text_color, font=font)\n",
    "    return img\n",
    "\n",
    "# Step 5: Overlay Speaker Labels\n",
    "clips = [video]\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    start_time = pd.to_datetime(row['Start']).time()\n",
    "    end_time = pd.to_datetime(row['End']).time()\n",
    "\n",
    "    start_seconds = start_time.hour * 3600 + start_time.minute * 60 + start_time.second\n",
    "    end_seconds = end_time.hour * 3600 + end_time.minute * 60 + end_time.second\n",
    "\n",
    "    text_img = create_text_image(row['Speaker'])\n",
    "    text_img_path = '/content/temp_text_img.png'\n",
    "    text_img.save(text_img_path)\n",
    "\n",
    "    txt_clip = (ImageClip(text_img_path)\n",
    "                .set_position(('center', 'bottom'))\n",
    "                .set_start(start_seconds)\n",
    "                .set_duration(end_seconds - start_seconds))\n",
    "\n",
    "    clips.append(txt_clip)\n",
    "\n",
    "# Combine all clips\n",
    "final_video = CompositeVideoClip(clips)\n",
    "\n",
    "# Step 6: Save the Modified Video\n",
    "final_video_path = '/content/videoplayback_label.mp4'\n",
    "final_video.write_videofile(final_video_path, codec='libx264')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def show_video(final_video_path, video_width = 1000):\n",
    "\n",
    "  video_file = open(final_video_path, \"r+b\").read()\n",
    "\n",
    "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
    "\n",
    "show_video(final_video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
